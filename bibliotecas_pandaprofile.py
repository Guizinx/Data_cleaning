# -*- coding: utf-8 -*-
"""data_cleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jt7OdctjaEcvLhSEbApvkIxGuJ0Y2uEQ

# **Setup**
"""

!pip install sidetable
!pip install pandas-profiling==3.3.0

import numpy as np
import pandas as pd

import seaborn as sns
import matplotlib.pyplot as plt

import sidetable
import pandas_profiling
import missingno as msno
from ipywidgets import interact, widgets

from sklearn import datasets
from sklearn.preprocessing import scale, minmax_scale, power_transform

sns.set_theme(
    context='talk',
    style='ticks',
    font_scale=.8,
    rc={
        'figure.figsize': (12,8)
    }
)

def list_attributes(obj):
  """Lista atributos e funções de um objeto"""
  return [attr for attr in obj.__dir__() if not attr.startswith('_')]

"""# **Contextualização**

A limpeza dos dados, também chamada de **data cleaning**, ou mesmo data cleansing, consiste principalmente em tratar dados corrompidos ou inacurados, proveniente de erros, como por exemplo erros e gravação dos dados.

A estruturação ou manipulação dos dados, também chamada de **data wrangling** ou data munging, consiste em todo processo de transformação dos dados para a melhor estrutura possível para análise e modelagem dos dados.

Como o contexto do data wrangling é mais geral, podemos dizer que data cleaning é uma parte (com um foco em identificação e removação de erros) dentro do data wrangling, que é o processo geral para estruturar nossos dados de forma que possamos atingir nosso objetivo.

&nbsp;

---

&nbsp;

**O que os alunos devem esperar deste módulo?**

Neste módulo, dentro do contexto do data cleaning e data wrangling, aprenderemos:
- Como identificar e lidar com valores faltantes
- Como identificar e lidar com outliers
- Como identificar e lidar com dados inconsistentes através de análise univariada
- Como identificar variáveis potencialmente importantes para o modelo através da análise multivariada
- Como transformar seus dados numéricos e categóricos para aplicação do modelo
- Como estruturar os seus dados para seu objetivo final
- Como criar novas variáveis para o modelo
- Como estruturar um pipeline pré-tratamento dos dados

&nbsp;

---

&nbsp;

**O que esperamos dos alunos neste módulo?**

Esperamos que o aluno exerça a habilidade de **abstração** do que for ensinado enste módulo. A capacidade de abstrair um conteúdo, neste contexto, é ser capaz de utilizar o conteúdo aprendido neste módulo para outros contextos, outras bases de dados

**Tudo o que for ensinado neste módulo pode ser aplicado em diversos contextos e bases de dados diferentes.** Contudo, uma vez que temos alunos de distintos backgrounds, não é possível utilizar uma base de dados que contemple todas áreas, ou mesmo cada conteúdo deste módulo.

Outro ponto importante é o que aluno pratique o conteúdo aqui aprendido em seus próprios dados, em dados públicos, e que levante suas dúvidas para os mentores do curso sempre que necessário.

# **Bibliotecas facilitadoras de EDA**

Bibliotecas de "Auto EDA" que também facilitam na identificação de elementos para limpeza dos dados:

- [pandas-profiling](https://pandas-profiling.ydata.ai/docs/master/index.html)
- [dataprep](https://dataprep.ai/)
- [sweetviz](https://pypi.org/project/sweetviz/)
- [lux](https://lux-api.readthedocs.io/en/latest/source/getting_started/overview.html)
"""

# Read titanic dataset
df = pd.read_csv('https://raw.githubusercontent.com/adamerose/datasets/master/titanic.csv')
df.head()

df.profile_report()

"""# **Valores duplicados**"""

# Identificar valores duplicados
df[df.duplicated(keep=False)]

# Identificar quantos valores duplicados há em cada duplicata
(
  df
 .groupby(df.columns.tolist(), dropna=False)
 .size()
 .to_frame('n_duplicates')
 .query('n_duplicates>1')
 .sort_values('n_duplicates', ascending=False)
 .head(5)
)

# Remover valores duplicados
df.drop_duplicates()

"""# **Tratamento de valores nulos**

## Contextualização teórica

A maioria dos modelos de machine learning não conseguem trabalhar com valores faltantes, de forma que temos que tratá-los antes de aplicar os modelos. 

As duas abordagens mais comuns para tratamento de nulos é:
- Removê-los
- Imputá-los

Para remover os nulos, podemos remover as observações (ou seja, linhas) onde há valores nulos, assim perdendo a observação como um todo, ou, ainda remover a variável (coluna) como muitos valores faltantes, perdendo assim por completo a variável.

Esta abordagem de remoção de nulos costuma ser viável quando temos um dataset grande e com poucos valores nulos, de forma que a remoção de algumas linhas com valores nulos não impacta na acurácia do modelo que desejamos utilizar.
"""

# Commented out IPython magic to ensure Python compatibility.
# #@title
# %%html
# <style>
# div.warn {    
#     color: #FE4754;
#     background-color: #FFE1E3;
#     border-left: 5px solid #FE4754;
#     padding: 0.5em;
#     }
#  </style>
# <div class=warn>
# <b>Sempre se questione do motivo dos nulos!</b><br/><br/>
# <div style="color: black">
# Como cientista de dados, não podemos simplesmente remover observações sem entender o motivo dos nulos:
# <li> É decorrente de erro?</li>
# <li> Pode significar algum valor? (neste caso faríamos uma imputação)</li>
# <li> Existe algum padrão nestes valores com nulos? (exemplo, ausente apenas para determinado grupo)</li>
# </div>
# </div>

"""Para a outra alternativa, a imputação de valores nulos, simplesmente preenchemos os valores nulos por algum valor determinado pelo próprio cientista, possivelmente com insight de uma pessoa de negócios.

Esse valor pode ser a média, mediana, moda ou outra métrica de agregação da coluna em questão. Também pode ser zeros, no caso de coluna numérica, ou uma categoria diferente como "OUTROS", no caso de colunas categóricas.

## Prática
"""

# Load titanic dataset
df = pd.read_csv('https://raw.githubusercontent.com/adamerose/datasets/master/titanic.csv')
df.head()

"""### Identificando dados nulos

Método mais comum de calcular e mostrar os valores nulos:
"""

# Identificar e mostrar valores nulos
df_missing = (
  df.isna().sum()
  .to_frame('missing_count')
  .join((df.isna().sum() / df.shape[0]).to_frame('missing_pct'))
  .sort_values('missing_count', ascending=False)
)

df_missing.style.format('{:.2%}', subset=['missing_pct'])

"""Método com sintaxe em cadeia:"""

df.isna().sum().to_frame('missing_count').assign(missing_pct = lambda x: x.div(len(df))).sort_values('missing_count', ascending=False)

"""Utilizando a biblioteca `sidetable`:"""

df.stb.missing()

"""Utilizando a biblioteca `missgno`:"""

msno.matrix(df);

"""Mostrando as linhas com valores faltantes como um todo:"""

df[df.isna().any(axis=1)]

"""Mostrando as linhas com valores faltantes na coluna 'embarked':"""

df[df.embarked.isna()]

"""### Removendo valores nulos"""

# Removendo coluna deck
df.drop(columns=['deck'])

# Valor máximo de NAs permitido
max_nan_freq = .7
# Identificando colunas
na_cols = df.isna().sum().div(len(df))
cols2drop = na_cols[na_cols>max_nan_freq].index

# Dropando colunas com muitos NAs
df.drop(columns=cols2drop)

# Dropando colunas inplace
df.drop(columns=cols2drop, inplace=True)

df.head()

"""### Imputando valores nulos"""

df.tail()

"""Imputando valores nulos com média/mediana/moda:"""

df.fillna({'age': df.age.mean(), 'embarked': df.embarked.mode()[0]}).tail()

"""Imputando valores nulos com valores passados, futuros, ou via interpolação:"""

(
  df
 .assign(
     age_bfill = df.age.bfill(),
     age_ffill = df.age.ffill(),
     age_interpolate = df.age.interpolate()
 )
 .filter(like='age')
 .tail()
 .style.highlight_null('yellow')
)

# Imputando NAs por grupo
df.groupby('sex')['age'].apply(lambda x: x.fillna(x.mean()))

"""Outros métodos de imputação:

- [~SimpleImputer~](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer)
- [IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer)
- [KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer)

# **Tratamento de outliers**

## Contexto teórico

Outliers são pontos muito distantes do centro de distribuição dos dados e impactam grande parte das análises e modelos de machine learning.

As causas de outlieres podem ser:
- Erro de mensuração/amostragem
- Outlier natural dos dados

Neste tópico aprenderemos como identificar e tratar outliers, que seja removendo-os ou aplicando transformação.

Cabe ressaltar que outliers naturais dos dados não devem ser removidos levianamente, ou seja, sem antes identificá-los e sem tentar entender seus motivos, se há padrões neste outliers. Uma vez identificado estes outliers e suas possíveis causas, e no caso de não haver problema/interesse em não considerá-los na análise, podemos removê-los.

## Prática

Para esta análise, vamos trabalhar com um dataset com vários atributos de vinhos.
"""

df,_ = datasets.load_wine(return_X_y=True, as_frame=True)
print(df.shape)
df.head()

"""### Identificando/removendo outliers

A maneira mais direta (e simples) de observar um outlier é através do gráfico boxplot:
"""

# Box plot com dados "as is"
df.plot.box()
plt.xticks(rotation=60, ha='right');

"""Conseguimos observar que há outliers em algumas variáveis, mas devido à diferença de escala entre as variáveis, fica difícil compará-las em um único plot. 

Uma alternativa aqui seria plotar separadamente. Contudo, podemos optar também por padronizar estes dados apenas para visualização usando a função `scale` da importada da biblioteca `sklearn.preprocessing`:
"""

# Box plot com dados padronizados
df.apply(scale).plot.box()
plt.xticks(rotation=60, ha='right');

"""Verificamos que existem outliers, logo, o próximo passo seria identificá-los para verificar se faz sentido ou não. Para isso, discutiríamos com o time de negócios (neste caso, um especialista em vinhos). 

Uma vez que este dataset está sendo utilizado para fins didáticos, vamos considerar inicialmente que podemos excluir estes dados, e mais adiante veremos alternativas para prosseguir com a análise mesmo com estes outliers.

#### Interquartile range

Podemos identificar os outliers pelo método interquartil, utilizando a seguinte fórmula:

$$
\text{lower_outlier} = Q1 - 1.5 \times IQR
$$
$$
\text{upper_outlier} = Q3 + 1.5 \times IQR
$$

onde $Q1$ represente o primeiro quartil, $Q3$ o terceiro, e $IQR$ o intervalo interquartil. 

Segue abaixo uma possível implementação desta fórmula:
"""

def is_outlier(array, extreme=False):
  """Custom function to identify outliers in an array"""
  q1,q3 = np.quantile(array, [.25,.75])
  iqr = q3-q1
  
  factor = 3. if extreme else 1.5
  upper_outlier = q3 + factor*iqr
  lower_outlier = q1 - factor*iqr

  return (array < lower_outlier) | (array > upper_outlier)

"""A função retorna um array booleano do mesmo tamanho do array de entrada, indicando se o valor é ou não um outlier:"""

# Mostrando aplicação da função em uma dada coluna
is_outlier(df.ash)

# Calculando quantidade de outliers
is_outlier(df.ash).sum()

"""E podemos aplicá-la para uma ou mais colunas através do método `apply`:"""

# Mostrando aplicação da função para todas colunas numéricas contínuas
df.apply(is_outlier)

# Calculando quantidade de outliers
df.apply(is_outlier).sum()

"""Para remover os outliers, podemos ignorar todas linhas que tem pelo menos um `True` para outlier:"""

# Removendo outliers
df[~df.apply(is_outlier).any(axis=1)]

"""E podemos comparar as distribuições com ou sem remoção de outlier:"""

# Figura lado a lado dos dados originais e padronizados

df_without_outliers = df[~df.apply(is_outlier).any(axis=1)]

fig,axes = plt.subplots(ncols=2, figsize=(18,6))

df.apply(scale).plot.box(ax=axes[0], title='Boxplot sem filtro de outlier')
df_without_outliers.apply(scale).plot.box(ax=axes[1], title='Boxplot com filtro de outlier')

fig.autofmt_xdate(rotation=60, ha='right')
plt.show()

df_without_outliers.apply(is_outlier).sum()

"""Observe que, mesmo removendo os outliers, eles diminuem, mas não desapareceram! Por quê?

Ao adicionamos ou removemos valores acabamos modificando a distribuição dos dados, principalmente valores mais distantes do centro da distribuição. Por isso, a remoção de outliers não garante necessariamente que não terá mais outliers na nova distribuição. 

Neste caso, por exemplo, valores que previamente não eram outliers passaram a ser. Contudo, tais valores não estão tão distantes como os outliers originais.

#### Z-score

Outro método para identificar e também remover outliers é através da padronização Z-score, em que a média dos valores fica em 0, e o desvio padrão em 1. Usualmente, filtramos valores acima e abaixo de 3 desvios padrões (para cima e para baixo, respectivamente).

Nós já utilizamos o Z-score
"""

def zscore_outlier(array):
  scaled_array = scale(array)
  return (scaled_array<-3) | (scaled_array>3)

df_without_outliers = df[~df.apply(zscore_outlier).any(axis=1)]

# Figura lado a lado dos dados originais e padronizados destacando 3 std

fig,axes = plt.subplots(ncols=2, figsize=(18,6))

df.apply(scale).plot.box(ax=axes[0], title='Boxplot sem filtro de outlier')
df_without_outliers.apply(scale).plot.box(ax=axes[1], title='Boxplot com filtro de outlier')

fig.autofmt_xdate(rotation=60, ha='right')
plt.show()

"""### Corrigindo outliers

#### Cap

Um método para lidar com os outliers é substituí-los por um valor que faça sentido, como, por exemplo, substituir todos os valores que são maior do queo percentil 99% (ou seja, top 1% dos valores) pelo valor do percentil 99%.
"""

# Mostrando percentis
df.describe(percentiles=[.01, .25, .5, .75, .9, .99])

def cap_values(array, lower_quantile=0., upper_quantile=1.):
  array = array.copy()
  
  lower_quantile = np.quantile(array, lower_quantile)
  upper_quantile = np.quantile(array, upper_quantile)
  
  array[array<lower_quantile] = lower_quantile
  array[array>upper_quantile] = upper_quantile
  
  return array

# Mostrando percentis após aplicar função
cap_values(df.ash, upper_quantile=.90).max()

df.apply(cap_values, upper_quantile=.99).describe(percentiles=[.01, .25, .5, .75, .9, .99])

"""#### Transformações

##### Log
"""

# Figura lado a lado dos dados originais e padronizados destacando 3 std

fig,axes = plt.subplots(ncols=2, figsize=(18,6))

df.apply(scale).plot.box(ax=axes[0], title='Boxplot sem filtro de outlier')
df.apply(np.log).apply(scale).plot.box(ax=axes[1], title='Boxplot com transformação log')

fig.autofmt_xdate(rotation=60, ha='right')
plt.show()

"""##### Box-cox"""

from sklearn.preprocessing import power_transform

df_boxbox = pd.DataFrame(power_transform(df, method='box-cox'), columns=df.columns)

fig,axes = plt.subplots(ncols=2, figsize=(18,6))

df.apply(scale).plot.box(ax=axes[0], title='Boxplot sem filtro de outlier')
df_boxbox.plot.box(ax=axes[1], title='Boxplot com transformação Box-cox')

fig.autofmt_xdate(rotation=60, ha='right')
plt.show()

"""# Tratamento de datas

Para que o Python/Pandas reconheça que estamos trabalhando com o tipo de dado no formato data (`date` ou `datetime`), precisamos especificar este formato, para que assim possamos usufruir das diversas funções que lidam com este tipo de dado.

Para mostrar como podemos fazer essa conversão em uma tabela, iremos utilizar como exemplo dados de acidentes aéreos, neste momento selecionando apenas as colunas de data, que por padrão é lida como texto, e das fatalidades:
"""

df = pd.read_csv('https://query.data.world/s/2gfb7bmzhna6kcbpc7admwa6cexprz', usecols=['Date','Fatalities']).dropna()
print(df.shape)
df.head()

# Tipos de dados
df.info()

"""Observe que o tipo data type da coluna Date é `object`, e não `datetime`.

Uma alternativa seria ler a tabela já especificando as colunas que são datas, e o pandas vai tentar inferir o formato da data:
"""

# Formatando data automaticamente ao ler tabela
df = pd.read_csv('https://query.data.world/s/2gfb7bmzhna6kcbpc7admwa6cexprz', usecols=['Date','Fatalities'], parse_dates=['Date']).dropna()
df.info()

df.head()

"""Outra opção é converter o data type das colunas que representam data:"""

# Podemos permitir que o pandas infira automaticamente o formato da data se possível (padrão do Pandas)
# pd.to_datetime(df.Date)
df['Date'].astype('datetime64[ns]')

df.head(1)

# Especificando o formato (para dataset grande costuma ser inclusive mais rápido)
df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')
df.info()

"""Observe que podemos permitir ou não que o pandas faça a inferência do formato da data, contudo, é uma boa prática que especifiquemos o formato sempre que possível. Inclusive, para um grande volume de dados a conversão com espefificação tende a ser mais rápida.

Todos os símbolos dos formatos podem ser encontrados [neste link da documentação oficial do Python](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes).

Uma vez que a coluna de data esteja no formato certo, podemos utilizar o método `dt` do Pandas para acessar muitas funcionalidades úteis das datas/horários. Vide conjunto de funcionalidades na [documentação oficial](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.date.html).
"""

# Mostrando todos atributos e funções acessíveis pelo método dt
print(list_attributes(df['Date'].dt))

df['Date'].dt.day_of_week

"""Podemos colocar a data como índice no dataframe, e com isso acessar ainda mais funcionalidades ([vide documentação](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html)), como filtros mais efetivos e  agrupamentos inteligentes de períodos de tempo que será visto em conjunto com o `groupby` mais adiante."""

# Adicionando a data como index e ordenando-as
df_index = df.set_index('Date').sort_index()
df_index

# Filtrando do ano 2000 até o mês de maio de 2005
df_index.loc['2000-05-10':'2003-08']

# Agrupando por ano e somando as fatalidades
df_index.resample('YS').sum()

"""# Rolling Filters"""

df = pd.read_csv('https://query.data.world/s/2gfb7bmzhna6kcbpc7admwa6cexprz', usecols=['Date','Fatalities'], parse_dates=['Date'], index_col='Date')
df_monthly = df.resample('M').sum().sort_index()
df_monthly.head()

# Plot sem tratamento de ruídos com rolling filter
plt.figure(figsize=(18,7))
df_monthly.Fatalities.plot();

df_monthly.Fatalities.rolling(3).mean()



@interact(window_size=(12,121,12))
def rolling_mean(window_size):
  plt.figure(figsize=(18,7))
  df_monthly.Fatalities.rolling(window_size).mean().plot()
  plt.show()

rolling_mean(24)